{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import json\n",
    "#import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.ticker as ticker\n",
    "\n",
    "#from matplotlib.gridspec import GridSpec\n",
    "#from matplotlib.dates import DateFormatter\n",
    "#from matplotlib import dates\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import linear_model\n",
    "#from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "#from sklearn import neural_network\n",
    "#from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.base import BaseEstimator\n",
    "#from sklearn.base import ClassifierMixin\n",
    "#from sklearn.base import clone\n",
    "#from sklearn.pipeline import _name_estimators\n",
    "#from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from itertools import product\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.ensemble import BaggingClassifier\n",
    "#from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "t2mfile     = '/Users/ryaneagan/MAST638/Project/MAST638_Data/ERA5/t2m_2003_2023.nc'\n",
    "sstfile     = '/Users/ryaneagan/MAST638/Project/MAST638_Data/ERA5/sst_2003_2023.nc'\n",
    "dlwfile     = '/Users/ryaneagan/MAST638/Project/MAST638_Data/ERA5/dlw_2003_2023.nc'\n",
    "windfile    = '/Users/ryaneagan/MAST638/Project/MAST638_Data/ERA5/wnd_2003_2023.nc'\n",
    "cloudfile   = '/Users/ryaneagan/MAST638/Project/MAST638_Data/ERA5/cld_2003_2023.nc'\n",
    "rhfile      = '/Users/ryaneagan/MAST638/Project/MAST638_Data/ERA5/rhp_2003_2023.nc'\n",
    "seaicefile  = '/Users/ryaneagan/MAST638/Project/MAST638_Data/seaice_2003_2023.nc'\n",
    "\n",
    "t2m_raw     = xr.open_dataset(t2mfile,decode_coords=\"all\")\n",
    "sst_raw     = xr.open_dataset(sstfile,decode_coords=\"all\")\n",
    "dlw_raw     = xr.open_dataset(dlwfile,decode_coords=\"all\")\n",
    "wind_raw    = xr.open_dataset(windfile,decode_coords=\"all\")\n",
    "cloud_raw   = xr.open_dataset(cloudfile,decode_coords=\"all\")\n",
    "rh_raw      = xr.open_dataset(rhfile,decode_coords=\"all\")\n",
    "seaice_raw  = xr.open_dataset(seaicefile,decode_coords=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the Feb 29th in leap years\n",
    "\n",
    "t2m_raw     = t2m_raw.sel(time=~((t2m_raw.time.dt.month == 2) & (t2m_raw.time.dt.day == 29)))\n",
    "sst_raw     = sst_raw.sel(time=~((sst_raw.time.dt.month == 2) & (sst_raw.time.dt.day == 29)))\n",
    "dlw_raw     = dlw_raw.sel(time=~((dlw_raw.time.dt.month == 2) & (dlw_raw.time.dt.day == 29)))\n",
    "wind_raw    = wind_raw.sel(time=~((wind_raw.time.dt.month == 2) & (wind_raw.time.dt.day == 29)))\n",
    "cloud_raw   = cloud_raw.sel(time=~((cloud_raw.time.dt.month == 2) & (cloud_raw.time.dt.day == 29)))\n",
    "rh_raw      = rh_raw.sel(time=~((rh_raw.time.dt.month == 2) & (rh_raw.time.dt.day == 29)))\n",
    "seaice_raw  = seaice_raw.sel(time=~((seaice_raw.time.dt.month == 2) & (seaice_raw.time.dt.day == 29)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the wind magnitude and wind direction\n",
    "\n",
    "wind_raw['magnitude'] = np.sqrt(np.square(wind_raw['u10']) + np.square(wind_raw['v10']))\n",
    "wind_raw['direction'] = np.rad2deg(np.arctan2(wind_raw['v10'],wind_raw['u10']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weith the grid cells \n",
    "weights = np.cos(np.deg2rad(t2m_raw.latitude))\n",
    "weights.name='weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject the seaice data from the ease gride to WGS 1984 / epsg:4326 to match ERA 5 data\n",
    "seaice_reproj = seaice_raw.rio.reproject('epsg:4326')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight the cells for the ice data \n",
    "\n",
    "iweights = np.cos(np.deg2rad(seaice_reproj.y))\n",
    "iweights.name='weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the DDU area form the sea ice file and pull out only the mid concentation\n",
    "# smith area is 120E to 160E\n",
    "# DDU 139E, 146E\n",
    "\n",
    "seaice_area_mask = seaice_reproj.sel(y=slice(-60.0,-70.0),\n",
    "                                           x=slice(139.0,146.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "seaice_1d = seaice_area_mask['tc_mid'].where((seaice_area_mask['tc_mid'] <= 100.)).weighted(iweights).mean(dim=['x','y']).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaice_1d.drop(columns=['lambert_azimuthal_equal_area'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce spatial dimension\n",
    "\n",
    "t2m_1d      = t2m_raw.weighted(weights).mean(dim=['longitude','latitude']).to_dataframe()\n",
    "sst_1d      = sst_raw.weighted(weights).mean(dim=['longitude','latitude']).to_dataframe()\n",
    "dlw_1d      = dlw_raw.weighted(weights).mean(dim=['longitude','latitude']).to_dataframe()\n",
    "wind_1d     = wind_raw.weighted(weights).mean(dim=['longitude','latitude']).to_dataframe()\n",
    "cloud_1d    = cloud_raw.weighted(weights).mean(dim=['longitude','latitude']).to_dataframe()\n",
    "rh_1d       = rh_raw.weighted(weights).mean(dim=['longitude','latitude']).to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce temporal to weekly means\n",
    "\n",
    "t2m_weekly      = t2m_1d.resample('W').mean()\n",
    "sst_weekly      = sst_1d.resample('W').mean()\n",
    "dlw_weekly      = dlw_1d.resample('W').mean()\n",
    "wind_weekly     = wind_1d.resample('W').mean()\n",
    "cloud_weekly    = cloud_1d.resample('W').mean()\n",
    "rh_weekly       = rh_1d.resample('W').mean()\n",
    "seaice_weekly   = seaice_1d.resample('W').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed data for easier loading/processing later\n",
    "t2mfile_csv     = '/Users/ryaneagan/MAST638/Project/MAST638_Data/CSV/ddu_t2m_2003_2023.csv'\n",
    "sstfile_csv     = '/Users/ryaneagan/MAST638/Project/MAST638_Data/CSV/ddu_sst_2003_2023.csv'\n",
    "dlwfile_csv     = '/Users/ryaneagan/MAST638/Project/MAST638_Data/CSV/ddu_dlw_2003_2023.csv'\n",
    "windfile_csv    = '/Users/ryaneagan/MAST638/Project/MAST638_Data/CSV/ddu_wnd_2003_2023.csv'\n",
    "cloudfile_csv   = '/Users/ryaneagan/MAST638/Project/MAST638_Data/CSV/ddu_cld_2003_2023.csv'\n",
    "rhfile_csv      = '/Users/ryaneagan/MAST638/Project/MAST638_Data/CSV/ddu_rhp_2003_2023.csv'\n",
    "seaicefile_csv  = '/Users/ryaneagan/MAST638/Project/MAST638_Data/CSV/ddu_seaice_2003_2023.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_weekly.to_csv(t2mfile_csv)\n",
    "sst_weekly.to_csv(sstfile_csv)\n",
    "dlw_weekly.to_csv(dlwfile_csv)\n",
    "wind_weekly.to_csv(windfile_csv)\n",
    "cloud_weekly.to_csv(cloudfile_csv)\n",
    "rh_weekly.to_csv(rhfile_csv)\n",
    "seaice_weekly.to_csv(seaicefile_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadcsv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df['dateandtime'] = pd.to_datetime(df['time'])\n",
    "    df.set_index(['dateandtime'],inplace=True)\n",
    "    df.drop(columns=['time'],inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the files from CSV\n",
    "\n",
    "t2m_weekly = loadcsv(t2mfile_csv)\n",
    "sst_weekly = loadcsv(sstfile_csv)\n",
    "dlw_weekly = loadcsv(dlwfile_csv)\n",
    "wind_weekly = loadcsv(windfile_csv)\n",
    "cloud_weekly = loadcsv(cloudfile_csv)\n",
    "rh_weekly = loadcsv(rhfile_csv)\n",
    "seaice_weekly = loadcsv(seaicefile_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add weeks column for sorting and selecting later\n",
    "\n",
    "winter_weeks = [45,46,47,48,49,50,51,52,1,2,3,4,5,6,7,8]\n",
    "\n",
    "t2m_weekly['Week']      = t2m_weekly.index.isocalendar().week\n",
    "sst_weekly['Week']      = sst_weekly.index.isocalendar().week\n",
    "dlw_weekly['Week']      = dlw_weekly.index.isocalendar().week\n",
    "wind_weekly['Week']     = wind_weekly.index.isocalendar().week\n",
    "cloud_weekly['Week']    = cloud_weekly.index.isocalendar().week\n",
    "rh_weekly['Week']       = rh_weekly.index.isocalendar().week\n",
    "seaice_weekly['Week']   = seaice_weekly.index.isocalendar().week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_winter      = t2m_weekly[t2m_weekly['Week'].isin(winter_weeks)]\n",
    "sst_winter      = sst_weekly[sst_weekly['Week'].isin(winter_weeks)]\n",
    "dlw_winter      = dlw_weekly[dlw_weekly['Week'].isin(winter_weeks)]\n",
    "wind_winter     = wind_weekly[wind_weekly['Week'].isin(winter_weeks)]\n",
    "cloud_winter    = cloud_weekly[cloud_weekly['Week'].isin(winter_weeks)]\n",
    "rh_winter       = rh_weekly[rh_weekly['Week'].isin(winter_weeks)]\n",
    "seaice_winter   = seaice_weekly[seaice_weekly['Week'].isin(winter_weeks)]\n",
    "\n",
    "t2m_weekly.drop(columns=['Week'],inplace=True)\n",
    "sst_weekly.drop(columns=['Week'],inplace=True)\n",
    "dlw_weekly.drop(columns=['Week'],inplace=True)\n",
    "wind_weekly.drop(columns=['Week'],inplace=True)\n",
    "cloud_weekly.drop(columns=['Week'],inplace=True)\n",
    "rh_weekly.drop(columns=['Week'],inplace=True)\n",
    "seaice_weekly.drop(columns=['Week'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_winter.drop(columns=['Week'],inplace=True)\n",
    "sst_winter.drop(columns=['Week'],inplace=True)\n",
    "dlw_winter.drop(columns=['Week'],inplace=True)\n",
    "wind_winter.drop(columns=['Week'],inplace=True)\n",
    "cloud_winter.drop(columns=['Week'],inplace=True)\n",
    "rh_winter.drop(columns=['Week'],inplace=True)\n",
    "seaice_winter.drop(columns=['Week'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop outlier function\n",
    "\n",
    "def drop_outliers(df,varName,qmin,qmax):\n",
    "    s_qmin_max = df[varName].quantile([qmin,qmax])\n",
    "    newdf = df[(df[varName] > s_qmin_max[qmin]) & (df[varName] < s_qmin_max[qmax])]\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineDFS(dfs):\n",
    "    pdflist = dfs\n",
    "    from functools import reduce\n",
    "    df_merged = reduce(lambda  left,right: \n",
    "                       pd.merge(left,right,left_index=True,right_index=True,\n",
    "                                how='outer'), \n",
    "                       pdflist)\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_winter = drop_outliers(t2m_winter,'t2m',0.05,0.95)\n",
    "seaice_winter = drop_outliers(seaice_winter,'tc_mid',0.05,0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all of the individual dataframes into one dataframe for feature engineering\n",
    "\n",
    "dfs = [\n",
    "    t2m_weekly, \n",
    "    #sst_weekly,  \n",
    "    dlw_weekly,  \n",
    "    wind_weekly,  \n",
    "    cloud_weekly, \n",
    "    rh_weekly,\n",
    "    seaice_weekly\n",
    "]\n",
    "\n",
    "df = combineDFS(dfs)\n",
    "\n",
    "winter_dfs = [\n",
    "    t2m_winter,\n",
    "    #sst_winter,   \n",
    "    dlw_winter,   \n",
    "    wind_winter,   \n",
    "    cloud_winter,  \n",
    "    rh_winter, \n",
    "    seaice_winter \n",
    "]\n",
    "\n",
    "wdf = combineDFS(winter_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "wdf = wdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the columns we don't want\n",
    "#df.drop(columns=['u10','v10','z'],inplace=True)\n",
    "#wdf.drop(columns=['u10','v10','z'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with NAN\n",
    "\n",
    "df = df.dropna(axis=0)\n",
    "wdf = wdf.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add lagged ice cocentration columns\n",
    "df['tc_mid_1w'] = df['tc_mid'].shift(1)\n",
    "df['tc_mid_2w'] = df['tc_mid'].shift(2)\n",
    "df['tc_mid_4w'] = df['tc_mid'].shift(4)\n",
    "\n",
    "#drop the NAN rows created by the shift\n",
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lagged ice cocentration columns\n",
    "wdf['tc_mid_1w'] = wdf['tc_mid'].shift(1)\n",
    "wdf['tc_mid_2w'] = wdf['tc_mid'].shift(2)\n",
    "wdf['tc_mid_4w'] = wdf['tc_mid'].shift(4)\n",
    "\n",
    "#drop the NAN rows created by the shift\n",
    "wdf = wdf.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "zwdf = wdf.apply(zscore)\n",
    "bxax = sns.boxplot(data=zwdf,color='steelblue')\n",
    "bxax.set_xticklabels(bxax.get_xticklabels(),rotation=30)\n",
    "bxax.set(title='Z-Score Feature Box Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(zwdf[['t2m','sst','direction','magnitude','lcc','ssrd','r','tc_mid']])\n",
    "sns.pairplot(zwdf[['t2m','u10','v10','direction','magnitude','lcc','ssrd','r','tc_mid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr = zwdf[['t2m','sst','direction','magnitude','lcc','ssrd','r','tc_mid']].corr()\n",
    "#corr = zwdf[['t2m','u10','v10','direction','magnitude','lcc','ssrd','r','tc_mid']].corr()\n",
    "corr = zwdf[['t2m','u10','v10','direction','magnitude','lcc','ssrd','r','tc_mid_1w']].corr()\n",
    "sns.heatmap(corr, cmap = \"GnBu\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "corr[mask] = np.nan\n",
    "(corr\n",
    " .style\n",
    " .background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1)\n",
    " .highlight_null(color='#f1f1f1')  # Color NaNs grey\n",
    " .format(precision=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_plot(y_predicted, y_test, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y_predicted, y_test, edgecolors=(0, 0, 1))\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(title)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stats(y_predicted, y_test):\n",
    "    # model evaluation for testing set\n",
    "\n",
    "    mae = metrics.mean_absolute_error(y_test, y_predicted)\n",
    "    mse = metrics.mean_squared_error(y_test, y_predicted)\n",
    "    r2 = metrics.r2_score(y_test, y_predicted)\n",
    "    evar = metrics.explained_variance_score(y_test, y_predicted)\n",
    "\n",
    "    print(\"The model performance for testing set\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print('MAE is {}'.format(mae))\n",
    "    print('MSE is {}'.format(mse))\n",
    "    print('R2 score is {}'.format(r2))\n",
    "    print('Explained Variance Score is {}'.format(evar))\n",
    "    print('---------------------------------------\"\\n')\n",
    "    \n",
    "    return [mae,mse,r2,evar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = {'columns':['MAE','MSE','R2','ExpVarScore']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = [1.0,0.1,0.001,0.0001,0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor_cols = ['t2m','sst','direction','magnitude','lcc','ssrd','r']\n",
    "predictor_cols = ['t2m','u10','v10','direction','magnitude','lcc','ssrd','r']\n",
    "predictand_col = ['tc_mid_1w']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wdf[predictor_cols], wdf[predictand_col],test_size=0.3,random_state=5,shuffle=False)\n",
    "\n",
    "# trial OLS\n",
    "\n",
    "\n",
    "pipe_linear = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler',StandardScaler()),\n",
    "        ('ols',linear_model.LinearRegression()),\n",
    "\n",
    "    ]\n",
    ")\n",
    "pipe_linear.fit(X_train, y_train)\n",
    "y_pred = pipe_linear.predict(X_test)\n",
    "\n",
    "model = pipe_linear.named_steps['ols']\n",
    "coef = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=coef[0], palette='GnBu').set(title='OLS Feature Importance')\n",
    "\n",
    "regression_plot(y_pred,y_test,'OLS Regression')\n",
    "\n",
    "ols_stats = add_stats(y_pred,y_test)\n",
    "model_stats['ols'] = ols_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_elastic = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler',StandardScaler()),\n",
    "        ('elastic',linear_model.ElasticNet(alpha=1.0, l1_ratio=0.50))\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_elastic.fit(X_train, y_train)\n",
    "y_elastic_pred = pipe_elastic.predict(X_test)\n",
    "\n",
    "elastic_model = pipe_elastic.named_steps['elastic']\n",
    "\n",
    "elastic_coef = elastic_model.coef_\n",
    "elastic_intercept = elastic_model.intercept_\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=elastic_coef, palette='GnBu').set(title='ElasticNet Feature Importance')\n",
    "\n",
    "regression_plot(y_elastic_pred,y_test,'Elastic')\n",
    "elastic_stats = add_stats(y_elastic_pred,y_test)\n",
    "model_stats['elastic'] = elastic_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = ensemble.RandomForestRegressor()\n",
    "\n",
    "tree_model.fit(X_train, y_train)\n",
    "y_tree_pred = tree_model.predict(X_test)\n",
    "\n",
    "forest_importances = tree_model.feature_importances_\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=forest_importances, palette='GnBu').set(title='Random Forest Feature Importance')\n",
    "\n",
    "regression_plot(y_tree_pred,y_test,'Forest')\n",
    "\n",
    "forest_stats = add_stats(y_tree_pred,y_test)\n",
    "model_stats['forest'] = forest_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_ridgecv = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler',StandardScaler()),\n",
    "        ('ridgecv',linear_model.RidgeCV(alphas=alpha_range,cv=5))\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_ridgecv.fit(X_train,y_train)\n",
    "\n",
    "y_ridgecv_pred = pipe_ridgecv.predict(X_test)\n",
    "\n",
    "ridgecv_model = pipe_ridgecv.named_steps['ridgecv']\n",
    "\n",
    "ridgecv_alpha = ridgecv_model.alpha_\n",
    "ridgecv_coef = ridgecv_model.coef_\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=ridgecv_coef[0], palette='GnBu').set(title='RidgeCV Feature Importance')\n",
    "\n",
    "regression_plot(y_ridgecv_pred,y_test,'RidgeCV')\n",
    "ridgecv_stats = add_stats(y_ridgecv_pred,y_test)\n",
    "model_stats['ridgecv'] = ridgecv_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lassocv = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler',StandardScaler()),\n",
    "        ('lassocv',linear_model.LassoCV(alphas=alpha_range,cv=5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_lassocv.fit(X_train,y_train)\n",
    "\n",
    "lassocv_model = pipe_lassocv.named_steps['lassocv']\n",
    "\n",
    "y_lassocv_pred = pipe_lassocv.predict(X_test)\n",
    "\n",
    "lassocv_coef = lassocv_model.coef_\n",
    "lassocv_alpha = lassocv_model.alpha_\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=lassocv_coef, palette='GnBu').set(title='LassoCV Feature Importance')\n",
    "regression_plot(y_lassocv_pred,y_test,'LassoCV')\n",
    "\n",
    "lassocv_stats = add_stats(y_lassocv_pred,y_test)\n",
    "model_stats['lassocv'] = lassocv_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_elasticcv = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler',StandardScaler()),\n",
    "        ('elasticcv',linear_model.ElasticNetCV(alphas=alpha_range))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_elasticcv.fit(X_train,y_train)\n",
    "\n",
    "elasticcv_model = pipe_elasticcv.named_steps['elasticcv']\n",
    "\n",
    "y_elasticcv_pred = pipe_elasticcv.predict(X_test)\n",
    "\n",
    "elasticcv_alpha = elasticcv_model.alpha_\n",
    "elasticcv_coef = elasticcv_model.coef_\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=elasticcv_coef, palette='GnBu').set(title='ElasticNetCV Feature Importance')\n",
    "regression_plot(y_elasticcv_pred,y_test,'ElasticNetCV')\n",
    "\n",
    "elasticcv_stats = add_stats(y_elasticcv_pred,y_test)\n",
    "model_stats['elasticcv'] = elasticcv_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bayes = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler',StandardScaler()),\n",
    "        ('bayes',linear_model.BayesianRidge())\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_bayes.fit(X_train,y_train)\n",
    "\n",
    "y_bayes_pred = pipe_bayes.predict(X_test)\n",
    "\n",
    "bayes_model = pipe_bayes.named_steps['bayes']\n",
    "\n",
    "bayes_coef = bayes_model.coef_\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=bayes_coef, palette='GnBu').set(title='Bayes Feature Importance')\n",
    "\n",
    "regression_plot(y_bayes_pred,y_test,'Bayes')\n",
    "\n",
    "bayes_stats = add_stats(y_bayes_pred,y_test)\n",
    "model_stats['bayes'] = bayes_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_sgd = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler',StandardScaler()),\n",
    "        ('sgd',linear_model.SGDRegressor())\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_sgd.fit(X_train,y_train)\n",
    "\n",
    "y_sgd_pred = pipe_sgd.predict(X_test)\n",
    "\n",
    "sgd_model = pipe_sgd.named_steps['sgd']\n",
    "\n",
    "sgd_coef = sgd_model.coef_\n",
    "\n",
    "ax = sns.barplot(x = predictor_cols, y=sgd_coef, palette='GnBu')\n",
    "ax.set(title='SGD Feature Importance')\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)\n",
    "\n",
    "regression_plot(y_sgd_pred,y_test,'SGD Regression')\n",
    "\n",
    "sgd_stats = add_stats(y_sgd_pred,y_test)\n",
    "model_stats['sgd'] = sgd_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barfig, baxes = plt.subplots(4,2,figsize=(20,24))\n",
    "\n",
    "all_ax = baxes.ravel()\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=coef[0], palette='GnBu',ax=all_ax[0])\n",
    "all_ax[0].set(title='OLS Feature Importance')\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=forest_importances, palette='GnBu',ax=all_ax[1])\n",
    "all_ax[1].set(title='Forest Feature Importance')\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=ridgecv_coef[0], palette='GnBu',ax=all_ax[2])\n",
    "all_ax[2].set(title='RidgeCV Feature Importance')\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=lassocv_coef, palette='GnBu',ax=all_ax[3])\n",
    "all_ax[3].set(title='LassoCV Feature Importance')\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=elastic_coef, palette='GnBu',ax=all_ax[4])\n",
    "all_ax[4].set(title='Elastic Feature Importance')\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=elasticcv_coef, palette='GnBu',ax=all_ax[5])\n",
    "all_ax[5].set(title='ElasticCV Feature Importance')\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=bayes_coef, palette='GnBu',ax=all_ax[6])\n",
    "all_ax[6].set(title='Bayes Feature Importance')\n",
    "\n",
    "sns.barplot(x = predictor_cols, y=sgd_coef, palette='GnBu',ax=all_ax[7])\n",
    "all_ax[7].set(title='SGD Feature Importance')\n",
    "\n",
    "for j in range(len(all_ax)):\n",
    "    for i in all_ax[j].containers:\n",
    "        all_ax[j].bar_label(i,)\n",
    "\n",
    "barfig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = pd.DataFrame.from_dict(model_stats,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.rename(columns={0:'MAE',1:'MSE',2:'R2',3:'EVS'},inplace=True)\n",
    "\n",
    "ms['MAE'] = pd.to_numeric(ms['MAE'], errors='coerce')\n",
    "ms['MSE'] = pd.to_numeric(ms['MSE'], errors='coerce')\n",
    "ms['R2'] = pd.to_numeric(ms['R2'], errors='coerce')\n",
    "ms['EVS'] = pd.to_numeric(ms['EVS'], errors='coerce')\n",
    "\n",
    "ms.reset_index(drop=True)\n",
    "ms.drop(index=ms.index[0], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats_csv  = '/Users/ryaneagan/MAST638/Project/MAST638_Data/CSV/model_stats.csv'\n",
    "ms.to_csv(model_stats_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.count())\n",
    "print(X_test.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassocv_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree.plot_tree(tree_model, max_depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "def plot_permutation_importance(clf, X, y, ax):\n",
    "    result = permutation_importance(clf, X, y, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "    ax.boxplot(\n",
    "        result.importances[perm_sorted_idx].T,\n",
    "        vert=False,\n",
    "        labels=X.columns[perm_sorted_idx],\n",
    "    )\n",
    "    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html\n",
    "\n",
    "https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f\n",
    "\n",
    "https://medium.com/@hertan06/which-features-to-use-in-your-model-350630a1e31c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation feature importance for random forest\n",
    "\n",
    "mdi_importances = pd.Series(tree_model.feature_importances_, index=X_train.columns)\n",
    "tree_importance_sorted_idx = np.argsort(tree_model.feature_importances_)\n",
    "tree_indices = np.arange(0, len(tree_model.feature_importances_)) + 0.5\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "mdi_importances.sort_values().plot.barh(ax=ax1)\n",
    "ax1.set_xlabel(\"Gini importance\")\n",
    "plot_permutation_importance(tree_model, X_train, y_train, ax2)\n",
    "ax2.set_xlabel(\"Decrease in accuracy score\")\n",
    "fig.suptitle(\n",
    "    \"Impurity-based vs. permutation importances on multicollinear features (train set)\"\n",
    ")\n",
    "_ = fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poly regression\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "degree = 2 \n",
    "polynomial_features = PolynomialFeatures(degree = degree)\n",
    "\n",
    "x_train_poly = polynomial_features.fit_transform(X_train)\n",
    "x_test_poly = polynomial_features.fit_transform(X_test) \n",
    "print(x_train_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmodel = linear_model.LinearRegression() \n",
    "pmodel.fit(x_train_poly, y_train) \n",
    "y_poly_pred = model.predict(x_train_poly) \n",
    "\n",
    "#---plot the points--\n",
    "plt.scatter(X_train, y_train, s=10) \n",
    "#---plot the regression line--\n",
    "plt.plot(X_train, y_poly_pred) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAST638",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
